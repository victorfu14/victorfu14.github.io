\documentclass{report}
\usepackage{amsmath,amssymb,amsthm,textcomp,gensymb,nccmath,mathrsfs}
\usepackage{mathtools}
\renewcommand{\qedsymbol}{$\blacksquare$}

\setlength{\topmargin}{0.5in}
\usepackage[margin=4cm]{geometry}
\usepackage{enumerate}

\usepackage{setspace}
\onehalfspacing
\usepackage{parskip}
\setlength{\parskip}{0.5em}
\usepackage[T1]{fontenc}
\usepackage{palatino}

% useful characters/operators
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\B}{\mathbb{B}}
\newcommand{\matP}{\mathbb{P}}
\newcommand{\matE}{\mathbb{E}}
\newcommand{\matS}{\mathbb{S}}
\newcommand{\matH}{\mathbb{H}}
\newcommand{\matT}{\mathbb{T}}
\newcommand{\st}{\ s.t.\ }
\newcommand{\ie}{\ i.e.\ }
\newcommand{\eg}{\ e.g.\ }
\def \diam {\operatorname{diam}}
\def \Hom {\operatorname{Hom}}
\def \id {\operatorname{id}}
\def \tr {\operatorname{tr}}
\def \rk {\operatorname{rk}}
\def \sp {\operatorname{span}}
\def \dist {\operatorname{dist}}
\def \intr {\operatorname{int}}
\def \sgn {\operatorname{sgn}}
\def \im {\operatorname{Im}}
\def \re {\operatorname{Re}}
\def \curl {\operatorname{curl}}
\def \divg {\operatorname{div}}
\def \GL {\operatorname{GL}}
\def \Aut {\operatorname{Aut}}
\def \per {\operatorname{per}}
\def \LE {\operatorname{LE}}
\def \indeg {\operatorname{indeg}}
\def \outdeg {\operatorname{outdeg}}
\def \Par {\operatorname{Par}}
\def \Gr {\operatorname{Gr}}
\def \del {\operatorname{del}}
\def \add {\operatorname{add}}
\newcommand{\qbin}[2]{\begin{bmatrix}{#1}\\ {#2}\end{bmatrix}_q}
\newcommand{\pdr}[2]{\dfrac{\partial #1}{\partial #2}}
\newcommand{\df}{\mathrm{d}}
\newcommand{\dr}[2]{\dfrac{\df #1}{\df #2}}
\newcommand{\inner}[2]{\left\langle #1, #2\right\rangle}
\newcommand{\gen}[1]{\left\langle #1 \right\rangle}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\set}[1]{\left\lbrace #1 \right\rbrace}
\newcommand{\norm}[1]{\left\| #1 \right\|}

% arrows and :=, =:
\makeatletter
\providecommand*{\twoheadrightarrowfill@}{%
  \arrowfill@\relbar\relbar\twoheadrightarrow
}
\providecommand*{\twoheadleftarrowfill@}{%
  \arrowfill@\twoheadleftarrow\relbar\relbar
}
\providecommand*{\xtwoheadrightarrow}[2][]{%
  \ext@arrow 0579\twoheadrightarrowfill@{#1}{#2}%
}
\providecommand*{\xtwoheadleftarrow}[2][]{%
  \ext@arrow 5097\twoheadleftarrowfill@{#1}{#2}%
}
\makeatother

\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\mathrel{\mathop:}}

% integral for measure theory
\newcommand{\lowerint}{\underline{\int_{\R^d}}}
\newcommand{\upperint}{\overline{\int_{\R^d}}}
\newcommand{\lint}[1]{\underline{\int_{\R^d}} #1 (x)dx}
\newcommand{\uint}[1]{\overline{\int_{\R^d}} #1 (x)dx}
\newcommand{\sint}[1]{\simp{\int_{\R^d} #1 (x)dx}}
\newcommand{\lesint}[1]{\int_{\R^d} #1 (x)dx}

% note taking
\newcommand{\fancyem}[1]{\underline{\textsc{#1}}}

% theorem style
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{proposition}{Proposition}[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{question}{Question}

% pseudocode and algorithms
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\counterwithin{algorithm}{section}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% for clearer reference
\usepackage{hyperref}
\newcommand{\corollaryautorefname}{Corollary}
\newcommand{\lemmaautorefname}{Lemma}
\newcommand{\definitionautorefname}{Definition}
\newcommand{\exampleautorefname}{Example}
\newcommand{\conjectureautorefname}{Conjecture}
\renewcommand{\subsectionautorefname}{Section}
\newcommand{\algorithmautorefname}{Algorithm}

% other styling
\usepackage{fancyvrb, fancyhdr}
\usepackage{tikz-cd}
\usepackage{tikz}
\PassOptionsToPackage{usenames, x11names}{xcolor}
\usepackage{tcolorbox}
\selectcolormodel{cmy}

\newcommand{\edge}{
    \begin{tikzcd}[cramped, sep=small, labels={font=\everymath\expandafter{\the\everymath\textstyle}}]
        u \arrow[r, "e", no head] & v
    \end{tikzcd}
}


\pagestyle{fancy}
\fancyhead[LO,L]{\leftmark}
\fancyhead[RO,R]{Yiwei Fu}
% \fancyhead[C]{}
\fancyfoot[CO,C]{\thepage}
\renewcommand{\sectionmark}[1]{\markboth{#1}{#1}}

\numberwithin{equation}{section}

\newcommand{\fnl}{\parbox[t]{0\linewidth}{}}
\newcommand*\ttlmath[2]{\texorpdfstring{$\boldsymbol{#1}$}{#2}}

\usepackage{epigraph}

% \epigraphsize{\small}% Default
\setlength\epigraphwidth{8cm}
\setlength\epigraphrule{0pt}

\usepackage{etoolbox}

\makeatletter
\patchcmd{\epigraph}{\@epitext{#1}}{\itshape\@epitext{#1}}{}{}
\makeatother

% combinatorics special
\usepackage{pgfopts}
\usepackage{ytableau}

\begin{document}
\title{Notes for EECS 550: Information Theory}
\author{Yiwei Fu, Instructor: David Neuhoff}
\date{FA 2022}
\maketitle


\tableofcontents
Office hours: 

\clearpage
\pagenumbering{arabic}

\chapter{Source Codes}

\section{Lossless Coding}
Lossless coding is a type of data compression.

\fancyem{Goal} to encode data into bits so that \begin{enumerate}
  \item bits can be decoded perfectly or with very high accuracy back into original data;
  \item we use as few bits as possible.
\end{enumerate}

We need to model for data, a measure of decoding accuracy, a measure of compactness.

\fancyem{Model for data}
\begin{definition}
  A \emph{source} is a sequence of i.i.d (discrete) random variables $U_1, U_2, \ldots$
\end{definition}
We would like to assume a known alphabet $A = \{a_1, a_2, \ldots, a_Q\}$ and known probability distribution either through probability mass functions $p_U(u) = \Pr[U = u]$.

\begin{definition}
  Source coding
\end{definition}

\fancyem{Performance measures}
A measure of compactness (efficiency)

\begin{definition}
  \emph{Encoding rate}, also called \emph{rate}, is the average number of encoded bits per data symbol.

  There are two versions of average rate: \begin{enumerate}
    \item Emprical average rate \[\langle r \rangle \defeq \lim_{N \to \infty} \frac{1}{N}\sum_{k=1}^N L_k(U_1, \ldots, U_k),\]
    \item Statistical average rate
    \[
      \overline{r} \defeq \lim_{N \to \infty} \frac{1}{N}\sum_{k=1}^N \mathbb{E}[L_k(U_1, \ldots, U_k)]
    \]
  \end{enumerate}
  where $L_K$ is the number of bits out of the encoder after $U_k$ and before $U_{k+1}$.
\end{definition}

\begin{definition}
  The per-letter frequency of error is defined as \[
  \langle F_{LE}\rangle \defeq \lim_{N \to \infty} \frac{1}{N} \sum_{k=1}^N I(\hat{U}_k=U_k)
\]
  and per-letter error probability is defined as \[
  p_{LE} \defeq \lim_{N \to \infty} \frac{1}{N} \sum_{k=1}^N \mathbb{E}[I(\hat{U}_k=U_k)] = \lim_{N \to \infty} \frac{1}{N} \sum_{k=1}^N \Pr(\hat{U}_k=U_k)
\]
\end{definition}



\subsection{Fixed-length to Fixed-length Block Codes (FFB)}

% \[
% \begin{tikzpicture}
%   \draw[] rectangle (0, 0) to (1, 1);
% \end{tikzpicture}  
% \]

characteristics

\begin{definition}
  A code is \emph{perfectly lossless} (PL) if the $\beta(\alpha(\underline{u})) = \underline{u}$ for all $\underline{u} \in A_U^k$ (the set of all sequences $u_1, \ldots, u_k$).
\end{definition}

In order to be perfectly loss, $\alpha$ must be one-to-one. Encode must assign a distinct codeword ($L$ bits) to each data sequences. rate = $L/K$.
We seek $R^*_{PL}(k)$ the smallest rate of any PL code.

Number of sequences of size $k = Q^k$, and number binary sequence of size $L = 2^L$. We need $2^L \gg Q^K$.

\[
\overline{r} = \frac{L}{k} \geq \frac{k\log_2Q}{k} = \log_2Q
\]
Choose $\left\lceil k\log_2Q\right\rceil$, then we have \[
  R^*_{PL}(k) = \frac{\left\lceil k\log_2Q\right\rceil}{k} \leq\frac{k\log_2Q + 1}{k} =\log_2 Q + \frac{1}{k}.
\]
\[
  \log_2 Q \leq R^*_{PL}(k) \leq \log_2 Q + \frac{1}{k}
\]

Let $R^*_{PL}$ be the least rate of any PL FFB code with any $k$. 
$R^*_{PL}(k) \to \log_2 Q$ as $k \to \infty$.

$R^*_{PL} = \inf_k R^*_{PL}(k)$

Now we want rate less and $\log_2 Q$ almost lossless codes.

$R^*_{AL} =  \inf \{r, \text{there is an FFB code with $\bar{r} \leq r$ and arbitrarily small $P_{LE}$}\}$

$ = \inf \{r, \text{there is an FFB code with $\bar{r} \leq r$ and $P_{LE} < \delta$ for all $\delta > 0$}\}$

Instead of per-letter probability $P_{LE}$, we focus on block error probability $P_{BE} = \Pr(\underline{\hat{U}} \neq \underline{U})$

\begin{lemma}
  $P_{BE} \geq P_{LE} \geq \frac{P_{BE}}{k}$
\end{lemma}
\begin{proof}
  See homework.
\end{proof}

To analyze, we focus on the set of correctly encoded sequences. $G = \{\underline{u}: \beta(\alpha(\underline{u})) = \underline{u}\}$

Then we have \[P_{BE} = 1 - \Pr[U \in G], |G| \leq 2^k, L \geq \ceil{\log_2|G|}.\]

\fancyem{Question} How large is the smallest set of sequences with length $k$ form $A_U$ with probability $\approx 1$? 

We need to use weak law of large numbers (WLLN).

\begin{theorem}
  Suppose $A_x = \{1, 2, \ldots, Q\}$ with probability $p_1, \ldots, p_Q$. Given $\underline{u} = (u_1, \ldots, u_k) \in A^k_U$.
  \[
    n_q(\underline{u}) \defeq \# \text{times $a_q$ occurs in $\underline{u}$}, \quad f_q(\underline{u}) = \frac{n_q(\underline{u})}{k} = \text{frequency}
  \]
  Fix any $\varepsilon > 0$, \[
    \Pr[f_q(\underline{u}) \doteq p_q \pm \varepsilon] \to 1 \text{ as } k \to \infty.
  \] Moreover,
  \[
    \Pr[f_q(\underline{u}) \doteq p_q \pm \varepsilon, q = 1, \ldots, Q] \to q \text{ as } k \to \infty.  
  \]
\end{theorem}
\fancyem{Notation} $a \doteq b \pm \varepsilon \iff |a - b| \leq \varepsilon$

Consider subset of $A^k_U$ that corresponds to this event $x$.

$T_k = \{\underline{u}: f_q(\underline{u}) \doteq p_q \pm \varepsilon, q = 1, \ldots, Q\}$.

$\Pr[\underline{U} = \underline{u}] = p(u_1)p(u_2)\ldots p(u_k)$.

By WLLN, $\Pr(T_k) \to 1$ as $k \to \infty$.

\fancyem{Key Fact} all sequences in $T_k$ have approximately the same probability.

For $\underline{u} \in T_k$,
\begin{align*}
  p(\underline{u}) & = p(u_1)p(u_2)\ldots p(u_k) \\
  & = p_1^{n_1(u)}p_2^{n_2(u)}\ldots p_k^{n_k(u)} \\
  & = p_1^{kf_1(u)}p_2^{kf_2(u)}\ldots p_k^{kf_k(u)} \\
  & \approx \tilde{p}^k \text{ where } \tilde{p} = p_1^{p_1}p_2^{p_2} \ldots p_Q^{p_Q}.
\end{align*}
So we have $|T_k| \approx \frac{1}{\tilde{p}^k}$.

Then we have \[
  \overline{r} = \frac{\log_2|T_k|}{k} = -\frac{k\log_2\tilde{p}}{k} = -\log_2\tilde{p}. 
\]

Is that rate good? Can we do better? Can we have a set $S$ with probability $\approx 1$ and significantly smaller?

Since $\Pr(\underline{U} \in A^k_U \setminus T_k) \approx 0 \implies \Pr(\underline{U} \in S) \approx \Pr(\underline{U} \in S \cap T_k) \approx \frac{|S|}{|T_k|}$. So when $k$ is large, $T_k$ is the smallest set with large probability. And $R^*_{AL} \approx -\log \tilde{p}$.

How to express $\tilde{p}$.

\begin{align*}
  -\log \tilde{p} & = -\log \prod_{i=1}^Q p_i^{p_i} \\
  & = -\sum_{i=1}^Q p_i\log p_i \eqdef \text{entropy} = H.
\end{align*}

Some properties of $H$:
\begin{enumerate}
  \item its unit is bits
  \item $H \geq 0$.
  \item $H = 0 \iff p_q = 1$ for some $q$.
  \item $H \leq \log_2 Q$.
  \item $H = \log_2 Q \iff p_q = \frac{1}{Q}$ for all $q$.
\end{enumerate}


Identify the set that WLLN says has probability $\to 1$. Suppose $X_1, X_2, \ldots$ i.i.d. real-valued variables. \[
  T = \{\underbrace{x_1 \ldots x_n}_{\underline{x}} \in A_X^N: \frac{1}{N}\sum_{i=1}^N x_i \doteq \overline{x} \pm \varepsilon\}  
\] is called a typical set. $\Pr(\underline{X} \in T) \approx 1$ when $N$ is large.

Now suppose $X_1, X_2, \ldots$ i.i.d. $A_x$-valued random variables, function $g: A_x \to \R$. Consider $Y_1, Y_2, \ldots$ with $Y_i = g(X_i)$. $Y_i$'s are i.i.d. random variables. 

If $\matE[g(X)]$ is finite than we can apply WLLN that \[
  \Pr\left(\frac{1}{N}\sum_{i=1}^{N} Y_i \doteq \matE[Y] \pm \varepsilon\right) \to 1 \quad \text{as} \quad N \to \infty.
\]

Typical sequences wrt $g$: \[
  T^N_{x, p_\lambda, g, \varepsilon} = \left\lbrace\underline{x}: \frac{1}{N}\sum_{i=1}^N g(x_i) \doteq \overline{g(X)} \pm \varepsilon\right\rbrace. 
\]
If $\matE[g(X)]$ is finite then by WLLN we have \[
  \Pr(\underline{X} \in T_g) \to 1 \quad \text{as} \quad N \to \infty.  
\]

\begin{example}[Indicator function]
  Suppose $F \subset A_X$, and $g(x) = \begin{cases}
    1 & x \in F \\
    0 & x \notin F
  \end{cases}$. Then $\frac{1}{N}\sum_{i=1}^N g(x_i) = f_F(x)$. Now \[
    T_g = \{\underline{x}: f_F(x) \doteq \Pr(X \in F) \pm \varepsilon\}.
  \] By WLLN,
  \[
    \Pr(\underline{X} \in T_g) \to 1 \quad \text{as} \quad N \to \infty, \implies \Pr(n_F(\underline{X}) \doteq \matE[x] \pm \varepsilon) \to 1.  
  \]
\end{example}
\begin{example}
  $A_x = \R, g(x) = x^2$. $T_g = \{\underline{x}: \}$
\end{example}

\begin{theorem}
Now suppose $M$ functions $g_1, g_2, \ldots, g_M$. Fix $\varepsilon$. Then \[
  T_{g_1, g_2, \ldots, g_M} = \bigcap_{i=1}^M T_{g_i}.
\]
\[
  \Pr(\underline{X} \in T_{g_1, g_2, \ldots, g_M}) \to 1 \quad \text{as} \quad N \to \infty.
\] 
\end{theorem}
\begin{proof}
  \begin{align*}
    \Pr(\underline{X} \notin T_{g_1, g_2, \ldots, g_M}) & = \Pr\left(\underline{X} \in \left( \bigcap_{i=1}^M T_{g_i}\right)^c\right) \\
    & = \Pr\left(\underline{X} \in \left( \bigcup_{i=1}^M T_{g_i}^c\right)\right) \\
    & \leq \sum_{i=1}^M \Pr(\underline{X} \in T_{g_i}^c) \to 0 \quad \text{as} \quad N \to \infty. \qedhere
  \end{align*}
\end{proof}

\fancyem{Important Application}

Suppose $A_x = \set{a_1, \ldots, a_Q}$ a finite alphabet with probability $p_1, \ldots, p_Q$. The $g_q(x)$ be the indicator of $a_q$. $T_q = \set{\underline{x}: f_q(\underline{x})\doteq p_q \pm \varepsilon}$. And $\tilde{T} = \bigcap_{i=1}^Q T_i = \set{\underline{x}: \forall q, f_q(x) \doteq p_q \pm \varepsilon}$. $\tilde{T}_{X, p_X, \varepsilon}^N$ very typical sequence. We have \[
  \Pr(\underline{X} \in \tilde{T}) \to 1 \quad \text{as} \quad N \to \infty. 
\]

If $\underline{x} \in \tilde{T}$, then $\underline{x} \in \tilde{T}_g$ for any other $g$. Consider any real-valued $g$. If $\underline{x} \in \tilde{T}_\varepsilon$ then $\underline{x} \in T_{g, \varepsilon c}$ for some $c$. \[
  \frac{1}{N}\sum_{i=1}^N g(x_i) = \sum_{q=1}^Q \frac{n_q(x)}{N}g(Q_q) = \sum_{q=1}^Q (p_q \pm \varepsilon)q(a_q) = \matE[g(X)] + \varepsilon\sum_{q=1}^Q g(Q_q)  
\]

$\Pr(\underline{X} \in \tilde{T}) \to 1$ as $N \to \infty$.

If $\underline{x} \in \tilde{T}$, \begin{align*}
  p(\underline{x}) & = p(x_1)p(x_2)\ldots p(x_N) \\
  & = p_1^{n_1(\underline{x})}\ldots \\
  & = p_1^{f_1(\underline{x})N}\ldots \\
  & \doteq p_1^{(p_1 \pm \varepsilon)N}\ldots \\
  & \doteq 2^{N\left(\sum_{q=1}^Q p_q\log p_q \pm \varepsilon \sum_{q=1}^Q\log p_q\right)}
  & \doteq 2^{-NH \pm N\varepsilon c}
\end{align*}

\begin{theorem}[Shannon-McMillian Theorem]
  Suppose $X_1, X_2, \ldots$ i.i.d, $A_x = \set{a_1, \ldots, a_Q}$ with probability $p_1, \ldots, p_Q$. Then \begin{enumerate}
    \item  \[ \Pr(\tilde{X} \in \tilde{T}_\varepsilon^N) \to 1 \text{ as } N \to \infty. \]
    \item If $\underline{x} \in \tilde{T}_\varepsilon^N$, $p(\underline{x}) \doteq 2^{-NH \pm N\varepsilon c}$.
    \item $\left|\tilde{T}_\varepsilon^N\right| \doteq \Pr(\underline{X} \in \tilde{T}_\varepsilon^N)2^{N(H \pm \varepsilon x)}$.
  \end{enumerate}
\end{theorem}
\begin{proof}
\end{proof}

\section{Shannon-McMillian Theorem}
Is $\tilde{T}$ essentially the smallest set with probability $\approx 1$?

Yes. Let $S \in A_x^N$. We have \[
  \Pr(\underline{X} \in S = \Pr(X \in S \cap \tilde{T}) + \Pr(X \in S \cap \tilde{T}^c) \ddot{=} |S \cap \tilde{T}|2^{-NH \pm 2N\varepsilon c} + \Pr(\tilde{T}^c) \to 0 \text{ as } N \to \infty.  
\]

\begin{theorem}
  For every $\varepsilon > 0$, there is a sequence $b_{\varepsilon,1}, b_{\varepsilon,2}, \ldots \st b_{\varepsilon, N} \to 0$ as $N \to \infty$, $b_{\varepsilon, B} \geq 0$.

  For any $N$ and any $S \subset A_X^N$, \[
    |S| \geq \left(\Pr(\underline{X}\in S) - b_{\varepsilon, N}\right)2^{NH - N\varepsilon c}.  
  \]
\end{theorem}

An in hindsight shortcut

Let us directly consider \begin{align*}
  T_{S, \varepsilon}^N & = \set{\underline{x}: p(\underline{x}) \doteq 2^{-N (H\pm\varepsilon)}} \\
  & = \set{\underline{x}: -\frac{1}{N}\log p(\underline{x}) \doteq H\pm\varepsilon} \\
  & = \set{\underline{x}: -\frac{1}{N}\sum_{i=1}^N\log p(x_i) \doteq H\pm\varepsilon}
\end{align*}

compare $\tilde{T}_\varepsilon^N$ and $T^N_{s, \varepsilon}$.

Claim: $\tilde{T}_\varepsilon^N \subset T^N_{s, \varepsilon}$ where $c = -;\sum_{q=1}^Q \log p_q$.

Suppose $\underline{x} \in \tilde{T}_\varepsilon^N$. Show if it is also in $T_{s, \varepsilon}^N$. Check the following $p(x) \doteq 2^{-NH \pm N\varepsilon c}$, $-\log p(x) \doteq NH \pm N\varepsilon c$.

\begin{align*}
  -\log p(\underline{x}) & = -\log\prod_{i=1}^N p(x_i) \\
  & = -\log \prod_{q=1}^Q p_q^{n_q(x)} \\
  & = -\log \prod_{q=1}^Q p_q^{Nf_q(x)} \\
  & \doteq -\log \prod_{q=1}^Q p_q^{N(p_q \pm \varepsilon)} \\
  & \doteq -\sum_{q=1}^Q N(p_q \pm \varepsilon) \log p_a \\
  & \doteq NH \pm N\varepsilon\sum_{q=1}^C \log p_k \\
  & \doteq NH \pm N\varepsilon c.
\end{align*}

Extreme example:

$A_x = \{0, 1\}, p_0 = p_1 = \frac{1}{2}$. $H = 1$.

$p(\underline{x}) = 2^{-N}$.

$T_{s, \varepsilon}^N = \set{\underline{x}: p(\underline{x}) = 2^{-N(H\pm \varepsilon)} = 2^{-N}} = A_X^N$.

$\tilde{T}_\varepsilon^N = \set{\underline{x}: n_1(\underline{x} \doteq N\left(\frac{1}{2} + \varepsilon\right))}$.

$|T_{s, \varepsilon}^N| \ddot{=} 2^{N(H \pm \varepsilon)}, |\tilde{T}_\varepsilon^N| \ddot{=} 2^{N(H \pm 2 \varepsilon c)}$.

$T_s$ is called probability typical. $\tilde{T}$ is called frequency typical.

Example $A_x = \{0, 1\}$, $p_1 = \frac{1}{4}, p_0 = \frac{3}{4}$. $\tilde{T}_\varepsilon^N = \set{\underline{x}: f_1(\underline{x}) \doteq \frac{1}{4} + \varepsilon}$.

$T_{s, \varepsilon}^T = \set{\underline{x}: f_1(\underline{x}) = \frac{1}{4} \pm N\varepsilon\log\frac{1 - p_1}{p_1}}$


Typical sequences for an infinite alphabet

There are two cases:
$A_x$ is countably infinite / random variables are continuous

In the first case, frequency typical approach doesn't work. Probabilistic typical approach works just as is. $H = -\sum_{q=1}^\infty p_q \log p_q$ can be infinite.

Let $S_{\delta, N} = $ size of the smallest set of $N$ sequences form $A_x$ with probability at least $1-\delta$. Then for any $0 < \delta < 1$ and any $h$, $\frac{S_{\delta, N}}{2^Nh} \to \infty$ as $N \to \infty$.

\section{Fixed Length to Variable Length (FVB) Lossless Source codes}

Recall that FFB perfectly lossless has $R^*_{PL} = \log_2|A_x|$, and FFB almost lossless has $R^*_{AL} = H$.

FVB perfectly lossless $R^*_{VL} \leq \log_2|A_x|$.

Suppose we have a source with $A_x = \set{a,b,c,d}$ with probability $\set{\frac{1}{2},\frac{1}{4},\frac{1}{8},\frac{1}{8}}$.
\[\begin{tabular}{cc|cccccc}
  $p(u)$ & $u$ & code1 & code2 & code3 & code4 & code5 & code6\\ \hline
  $\frac{1}{2}$ & $a$ & 00 & 0 & 0 & 0 & 0 & 0 \\ 
  $\frac{1}{4}$ & $b$ & 01 & 10 & 10 & 10 & 1 & 01 \\ 
  $\frac{1}{8}$ & $c$ & 10 & 110 & 10 & 11 & 01 & 011\\
  $\frac{1}{8}$ & $d$ & 11 & 111& 11 & 111 & 10 & 0111\\ \hline
  & Rate & 2 & 1.75 & 1.5 & 1.625 & 1.25 & 1.875
\end{tabular}\]
We can see that code 3-5 are all bad.

Code 6 has an advantage that you know 0 represents the start of a codeword. We will see later why (\autoref{ex:onebitmess}).

FVB source code is characterized by \begin{itemize}
  \item source length $k$
  \item codebook of binary codewords $C = \set{\underline{v}_1, \underline{v}_2, \ldots, \underline{v}_{Q^K}}$, $Q = |A_U|$.
  \item encoding rule $\alpha: A_U^K \to C$
  \item decoding rule $\beta: C \to A_U^K$.
\end{itemize}

The encoder operates in block fashion. The decoder does not.

Distinguish codes that look like code2 and codes that look like code6.

\begin{definition}
  A codebook $C$ is \emph{prefix-free} if no codeword is the prefix of another. 
\end{definition}
A prefix-free code is called a prefix code. We will stick to prefix codes until states otherwise.
(instantaneously decodable)

We like to draw binary tree diagrams of code.

Code 1: \[
  233
\]

A prefix is perfectly lossless if and only if $\alpha$ is $1$-to-$1$.
The rate: $\overline{r}(c) = \frac{\overline{L}}{K} = \frac{1}{K}\sum_{\underline{u}}p(\underline{u})L(\underline{u})$ (length of codeword assigned to $\underline{u}$)

\[
  R^*_{VL}(k) = \min \set{\overline{r}(c): c \text{ is perfectly lossless FVB with source length $k$}}.
\]
\[
  R^*_{VL} = \inf \set{\overline{r}(c): c \text{ is PL FVB prefix code with any source length}} = \inf_K R^*_{VL}(k).
\]

How does one design a prefix code to have small or smallest rate?

Focus first $k = 1$. Shannon's idea: $L_q \approx -\log_2 p_q$.

$\sum_{q=1}^Q p_q L_q \approx -\sum_{q=1}^Q p_q \log p_q = H$.

\begin{question}
  Is there a prefix code with $L_q \approx -\log p_q$ for $q = 1, 2, \ldots, Q$? Could there be prefix codes with even smaller rate?
\end{question}

\begin{theorem}[Kraft inequality theorem]
  There is a binary prefix code with length $L_1, L_2, \ldots, L_Q$ iff the Kraft sum \[\sum_{q=1}^Q 2^{-L_q} \leq 1.\]
\end{theorem}
\begin{proof}
  Suppose $\underline{v}_1, \ldots, \underline{v}_Q$ is a prefix code with length $L_1, \ldots, L_Q$. Let $L_{\max} = \max_q L_q$.

  From the tree, the number of sequences of length $L_{\max}$ prefixed by any codeword, is $\sum_{q=1}^Q 2^{L_{\max} - L_q} \leq 2^{L_{\max}} \implies \sum_{q=1}^Q 2^{-L_q} \leq 1$. So the Kraft inequality holds.
\end{proof}

Now suppose
\begin{equation}\label{eq:shannon-fano}
  L_q = \ceil{-\log_2 p_q}, q = 1, \ldots, Q.  
\end{equation}
Is there a code with these lengths? Check Kraft.

\begin{align*}
  \sum_{q=1}^Q 2^{-L_q} & = \sum_{q=1}^Q 2^{-\ceil{-\log p_q}} \\
  & \leq \sum_{q=1}^Q 2^{-(-\log p_q)} \\
  & \leq \sum_{q=1}^Q 2^{\log p_q} \\
  & \leq \sum_{q=1}^Q p_q = 1.
\end{align*}
So the Kraft inequality holds. $\exists$ a prefix code with length $L_1, \ldots, L_Q$ given by \eqref{eq:shannon-fano}, called Shannon-Fano code.

Now the question is how good is this Shannon-Fano Code?

For the Shannon-Fano code, the rate (average length) is \[
  \overline{L}_{SF} = \sum_{q=1}^Q p_q\ceil{-\log p_q}.  
\]
We have the following bounds:
\[
  H = \sum_{q=1}^Q p_q(-\log p_q) \leq \overline{L}_{SF} < \sum_{q=1}^Q p_q(-\log p_q + 1) = H+1.
\]

\begin{question}
  Can we do better now?
\end{question}
We will show that $\overline{L} \geq H$ for any prefix code.

Let $C$ be a prefix code with length $L_1, \ldots, L_Q$. Take the difference $\overline{L}- H = \sum_{q=1}^Q p_qL_q + \sum_{q=1}^Q p_q \log p_q$.
\begin{align*}
  \overline{L} - H & = \sum_{q=1}^Q p_qL_q + \sum_{q=1}^Q p_q \log p_q \\
  & = -\sum_q p_q \log \frac{2^{-L_q}}{p_q} \\
  & = -\sum_q p_q \ln \frac{2^{-L_q}}{p_q} \frac{1}{\ln(2)} \\
  & \geq -\sum_q p_q \left(\frac{2^{-L_q}}{p_q} - 1\right)\frac{1}{\ln(2)}  \\
  & \geq -\frac{1}{\ln(2)}\sum_q 2^{-L_q} + \sum_q p_q \frac{1}{\ln(2)} = \frac{1}{\ln 2}(1 - 1) = 0.
\end{align*}

In homework we will that that $\overline{L}$ can get very close to $H + 1$.

Now allow $k \geq 1$. WE have a $C=\set{\underline{v}_1, \ldots, \underline{v}_{Q^k}}$ of length $L_1, \ldots, L_{Q^k}$. We want small \[
  \overline{r}(c) = \frac{\overline{L}}{K} = \frac{\sum_u p(\underline{u})L(\underline{u})}{k}.  
\]
Shannon-Fano code achieve that \[
  H^k \leq \overline{L}_{SF} < H^k + 1 \implies \frac{H^k}{k} \leq \overline{r}_{SF} = \frac{\overline{L}_{SF}}{k} < \frac{H^k}{k} + \frac{1}{k}.  
\]
Since $H^k = kH$ we have \[
  H \leq \overline{r}_{SF} < H^k + \frac{1}{k}.
\]
Similarly we have for any prefix code, we have \[
  \overline{r} = \frac{\overline{L}}{K} \geq \frac{H^k}{k} = H.
\]

This leads to a new coding theorem.
\begin{theorem}
  Given i.i.d. source $U$ with alpha $A_U$ and entropy $H$. We have \begin{enumerate}
    \item For every $k$, $R_{VL}^* \leq R_{VL}^*(k) < H + \frac{1}{k}$.
    \item For every $k$, $R_{VL}^*(k) \geq R_{VL}^* \geq H$.
  \end{enumerate}
  Combined we have \[
    \forall k \in \Z_{>0}, H \leq R_{VL}^*(k) < H + \frac{1}{k}
  \] and \[
    R_{VL}^* = H.  
  \]
\end{theorem}

\section{Huffman's Code Design}
Given $p_1, \ldots, p_Q$, it finds a prefix code with smallest $\overline{L}$.
\begin{algorithm}[h]
  \setstretch{1.15}
  \caption{Huffman Code}
  \label{algo:huffman}
  \begin{algorithmic}[1] 
    \Require Alphabet probability $\set{p_i | i = 1, \ldots, Q}$, WLOG assume $p_1 \geq p_2 \geq \ldots \geq p_Q$.
    \Ensure FVB Codebook for alphabet $\set{a_i | i = 1, \ldots, Q}$.
    \Function{Huffman}{$P_Q = \set{p_i | i = 1, \ldots, Q}$}
      \If{$Q = 2$}
        \Return $\set{0, 1}$
      \EndIf
      \State $p'_{Q-1} \gets p_{Q-1} + p_Q$
      \State $P_{Q-1} \gets \left(P_Q \setminus \set{p_{Q-1} + p_Q}\right) \cup \set{p'_{Q-1}}$
      \State $c_{Q-1} \gets \textsc{Huffman}(P_{Q-1}) \eqdef \set{\underline{v}_1, \ldots, \underline{v}_{Q-1}}$
      \State $c_Q \gets \set{\underline{v}_1, \ldots, \underline{v}_{Q-2}, \underline{v}_{Q-1}0, \underline{v}_{Q-1}1}$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{proposition}
  If $c_{Q-1}$ is optimal for $P_{Q-1}$ then $c_Q$ is optimal for $P_Q$.
\end{proposition}

\begin{example}
  
\end{example}

We found that \[\]

But there is a tighter upper bound \[
  \overline{L}^* \leq \begin{cases}
    H + p_{\max} & p_{\max} < \dfrac{1}{2} \\[0.8em]
    H + p_{\max} + 0.086 & p_{\max} \geq \dfrac{1}{2}.
  \end{cases}  
\]
Hence
\[
  \mathscr{R}_{VL}^*(k) \leq \begin{cases}
    H + \dfrac{p_{\max}^k}{k} & (p_{\max})^k < \dfrac{1}{2} \\[0.8em]
    H + \dfrac{p_{\max}^k}{k} + \dfrac{0.086}{k} & (p_{\max})^k \geq \dfrac{1}{2}.
  \end{cases}
\]

Up till now we've only focused on i.i.d RV's. Now suppose RV's are dependent, then \[\frac{H^k}{k} < \frac{kH}{k}.\]
For a stationary random process, \[
  \frac{H^k}{k} \searrow H_\infty.  
\] For example, English has \[
  H^1 \approx 4.08, H_\infty \approx 1.  
\]

The bits produced by a good lossless source code ($\overline{r} \approx H$) are approximately i.i.d. equiprobable.

Synchronization and transmission entropy

\begin{example}\label{ex:onebitmess}
  Suppose $\set{01, 001, 101, 110}$ for $\set{a, b, c, d}$. 
  \begin{align*}
    \underline{u} = ddddddddd\ldots \implies & \underline{z}= 110110110110110110\ldots \\
    \text{(if one leading $1$ is missing) } & \underline{z'} =101101101101\ldots \implies \hat{u} = ccccccc\ldots
  \end{align*}
  Now if $\set{1, 01, 001, 000}$ for $\set{a, b, c, d}$. Then the same problem will not happen.
\end{example}

\section{Buffering}
Suppose the source is outputting at $R$ symbols per second. The encoder would have $R\overline{r}$ bits per second.

Buffer overflow happens when a long sequence of low probability symbols are encoded.

Buffer underflow happens when a long sequence of high probability symbols are encoded. Buffer will be empty. Include an additional codeword in codebook called a ``flag''. Insert this codeword when buffer becomes empty.

We focused prefix codes. There are some non prefix codes that can be decoded losslessly.

\begin{definition}
  A code is \emph{separable} if any finite sequence of codewords is different from any other finite sequence of codewords.
\end{definition}

\begin{remark}
  Prefix codes are separable. And determining if a non prefix code is separable is not easy.
\end{remark}
Could prefix codes have smaller $I$?

McMillian's theorem says that Kraft inequality holds for separable codes. If you have a separable code whose codeword satisfy Kraft, then there is a prefix code with same lengths.

Lossless coding for source with infinite alphabet.

Suppose $A_n = \set{1, 2, 3, \ldots}$.
\begin{enumerate}
  \item FFB codes can't have finite rate if perfectly lossless
  \item AL FFB then SM theorem
  \item FVB. Current approach is based on Kraft inequality. It still holds for infinite case. (See Apppendix). But Huffman's optimal design does not apply.
\end{enumerate}

Other forms of variable length lossless source codes.
\begin{enumerate}
  \item Run-length coding
  \item Dictionary coding
\end{enumerate}

\section{Universal Source Coding}
Suppose you are to encode $10^6$ symbols from alphabet $A = \set{a, b, c, d}$. We can calculate $n_a(\underline{u}), n_b(\underline{u}), n_c(\underline{u}), n_d(\underline{u})$ and similarly, frequencies. Then we can apply Huffman or Shannon-Fano code.


\chapter{Entropy}
The star of this chapter is \[
  \ln x \leq x - 1  
\]

\section{Entropy}

Entropy \[
  H \defeq -\sum_{x} p(x)\log p(x)
\] is a measure of randomness or uncertainty.

\[
    H_q \defeq -\sum_{x} p(x)\log_q p(x),\quad H_q = H_r \frac{1}{\log_r q}.
\]

\[
  H(X) = \mathbb{E}[\log p_X(X)]  
\]
\begin{remark}
  \begin{enumerate}
    \item $H(X) \geq 0$ and $H(X) = 0 \iff p(x) = 1$ for some $x$.
    \item $H(X) = \infty$ if $X$ is continuous or has continuous component.
    \item $H(X, Y) \geq H(X)$ 
    \item $H(X, Y) = H(X) + H(Y)$ if $X$ and $Y$ are independent.
    \item $H(X_1, X_2, \ldots, X_n) = H(X_{\sigma(1)}, \ldots, X_{\sigma(n)})$ for any $\sigma \in S_n$.
  \end{enumerate}
\end{remark}

Divergence is a measure of dissimilarity of two probability distribution.
\begin{definition}
  Suppose $p$ and $q$ are probability mass functions. The \emph{divergence} from $p$ to $q$ is \[
    D(p \| q) = \sum_x p(x) \log \frac{p(x)}{q(x)}  
  \]
\end{definition}
\begin{remark}
  \begin{enumerate}
    \item $p = q\implies D(p \| q) = 0$
    \item It is \emph{not symmetric}. $D(p \| q) \neq D(q \| p)$. You can make symmetric by taking the sum, but then it is not nicely related to information theory.
  \end{enumerate}
\end{remark}

What if $p(x) = 0$ for some $x$? We take $0 \log \frac{0}{q(x)} = 0$. So if $\exists x \st q(x) = 0$ and $p(x) \neq 0$. Then $D(p \| q) = \infty$. 

When alphabet $A_x$ is infinite, $D(p \| q)$ can be $\infty$ even when $p(x) > 0$ and $q(x) > 0$ for all $x \in A_x$.

Is $\sum_{x} p(x) \log \frac{p(x)}{q(x)}$ is always defined? Write \[
  \sum_{x} p(x) \log \frac{p(x)}{q(x)} = \sum_{x, p(x) > q(x)}\log \frac{p(x)}{q(x)} + \sum_{x, p(x) < q(x)}\log \frac{p(x)}{q(x)}
\] We will show later that the second term is never $-\infty$, so it is always well-defined.

\begin{proposition}[Divergence inequality]
  For any $p, q$, \[D(p \| q) \geq 0, D(p \| q) = 0 \iff p = q.\]
\end{proposition}
\begin{proof}
  \begin{align*}
    D(p \| q) & = \sum_x p(x) \log \frac{p(x)}{q(x)} \\
    & = \sum_{x} p(x) \ln \frac{p(x)}{q(x)} \frac{1}{\ln 2} \\
    & = -\sum_{x} p(x) \ln \frac{q(x)}{p(x)} \frac{1}{\ln 2} \\
    & \leq -\sum_{x} p(x) \frac{q(x) - p(x)}{p(x)} \frac{1}{\ln 2} \\
    & \leq -\left(\sum_x p(x) - q(x)\right)\frac{1}{\ln 2} = 0
  \end{align*}
  For first equality, $\impliedby$ is clear. Now suppose $D(p \| q) = 0$. Then \[
    \ln \frac{q(x)}{p(x)} = \frac{q(x)}{p(x)} - 1 \implies p(x) = q(x) \text{for all $x$ with $p(x) > 0$}.
  \]
\end{proof}

Let's rewrite the divergence inequality a little bit.
\[
  0 \leq D(p \| q) = \sum_{x} p(x) \log \frac{p(x)}{q(x)} = -H(x) - \sum_x p(x) \log(q_x)  
\]
\[
  H(x) \leq - \sum_x p(x) \log(q_x)  
\] with $= \iff p = q$.
$- \sum_x p(x) \log(q_x) $ is called cross entropy. 
\begin{definition}
  The \emph{cross entropy} of $p$ with respect to $q$ is \[H_c(p, q) \defeq -\sum_x p(x) \log q(x).\]
\end{definition}

Cross-entropy inequality: for any $p, q$ \[
  H_p(X) \leq H_c(p, q)  
\] with $= \iff p = q$.

\begin{remark}
  \[
    D(p \| q) = H_c(p, q) - H_p(X) \iff H_c(p, q) = H_p(X) + D(p \| q).
  \]
\end{remark}

\begin{definition}
  Variation distance \[
    V(p, q) = \sum_x |p(x) - q(x)|  
  \]
\end{definition}
How does $D(p \| q)$ compare to $V(p, q)$?

\begin{proposition}[Pinskev's inequality]
  \[
    V(p, q) \leq \sqrt{(2 \ln 2)D(p \| q)}.
  \]
  So small $D(p \| q) \implies$ small $V(p,q)$ even when $|A_x| = \infty$. On the other hand the converse is not true.
\end{proposition}

\begin{lemma}
  If there exists $0 < \delta < 1$ such that $\frac{|p(x) - q(x)|}{p(x)} \leq \delta$ for all $x \st p(x) > 0$ then \[
    D(p \| q) \leq \frac{\delta}{1 - \delta}\frac{1}{\ln 2}  
  \]
\end{lemma}
If $D(p \| q) \approx 0$ then $p \approx q$, meaning $V(p, q) \approx 0$. 

If $p, q$ are percentage wise close, then $D(p \| q) \approx 0$.

Log-sum inequality

Suppose $u_1, u_2, \ldots, u_n$, $v_1, \ldots, v_n$ nonnegative. Then \[\sum_i u_i \log \frac{u_i}{v_i} \geq \left(\sum_i u_i\right)\log \frac{\sum_i u_i}{\sum_i v_i}.\]

This is a generalization of divergence inequality.


\section{Basic Properties of Entropy}

\begin{proposition}
  \[
    H(X^N) \leq \sum_{i=1}^N H(X_i)  
  \] with $= \iff X_1, \ldots, X_N$ are independent.  
\end{proposition}

\begin{proof}
  \begin{align*}
    H(X^N) \leq H_L(p, q)
  \end{align*}
  where $p$ is probability mass function of $X_1, \ldots, X_N$. Choose \[q(x_1, \ldots, x_n) = p(x_1)p(x_2)\ldots p(x_n).\]
\end{proof}

Since we are dealing with discrete random variables, it is useful to think about probability mass function as a set of probabilities $\set{p_1, p_2, \ldots}$. Write \[H(p_1, p_2, \ldots) = -\sum_i p_i \log p_i\]
Let $p'_i = p_i + p_j$, replace $p_i, p_j$ with $p'_i$ and leave all others the same. We have \[
-p_i \log p-i - p_j \log p_j \geq -(p_i + p_j)\log (p_i + p_j) 
\] i.e. entropy decreases when two probabilities are merged.

\begin{proposition}
  If $X$ is $Q$-ary with $Q < \infty$ then \[H(X) \leq \log_2 Q\]
\end{proposition}
\begin{proof}
  Let $q(x) = \frac{1}{Q}$.
  \begin{align*}
    H(X) \leq H_c(p, q) & = -\sum_x p(x) \log p(x) \\
    & = \sum_x p(x) \log_2 Q = \log_2 Q.
  \end{align*}
\end{proof}

\begin{proposition}
  Suppose $Y = g(X)$. Then \[
    H(Y) = H(g(X)) \leq H(X)
  \] and $= \iff g$ is one-to-one (probabilistically).
\end{proposition}
\begin{proof}
  \[
   H(Y) = \sum_{y} p(y) \log p(y)
  \] where $p(y) = \sum_{x, g(x) = y} p(x)$.
\end{proof}

$p_i = q^i(1 - q), i = 0, 1, 2, \ldots$, then \begin{align*}
  H(X) & = -\sum_{i=0}^\infty p_i \log p_i \\
  & = -\sum_{i=0}^\infty q^i(1-q)\log q^i(1-q) \\
  & = -\sum_{i=0}^\infty q^i(1-q)\left(i\log q + \log(1-q)\right) \\
  & = -\log q\sum_{i=0}^\infty q^i(1-q)i - \sum_{i=0}^\infty q^i(1-q)\log(1-q) \\
  & = -\log q \cdot q(1 - q) \dr{}{q} \sum_{i=0}^\infty q^i - \log(1-q) \\
  & = - \log q \cdot q(1 - q) \frac{1}{(1-q)^2} - \log(1-q) \\
  & =  - \log q \cdot \frac{q}{1-q} - \log(1-q) = \frac{-q\log(q) - (1 - q)\log(1-q)}{1- q} \\
  & = H(Q)\frac{1}{1 - q} < \infty.
\end{align*}

$p_i = \frac{\alpha}{i(\ln i)^2}, i = 2, 3, \ldots$, then \begin{align*}
  H(X) & = -\sum_{i=2}^\infty \frac{\alpha}{i(\ln i)^2} \log \left(\frac{\alpha}{i(\ln i)^2}\right) \\
  & = 
\end{align*}

\section{Conditional Entropy}

\[H(X \mid Y) = \sum_{x, y} p(x, y) \log p(x|y) \geq 0\] with equality iff $X$ is a function of $Y$.

\[H(X \mid Y) \leq H(X)\] with equality iff $X, Y$ are independent. 

chain rule: \[
  H(X, Y) = H(X) + H(Y \mid X) \implies H(X, Y) \geq H(X) 
\]

Conditional lossless source coding

\section{Convexity}
Goal: entropy is a concave (convex $\cap$).

Extended definition of entropy 
\begin{definition}
  \[\overline{H}(x) = \sup_{\text{finite quantizers $Q$}} H(Q(x)) \] where finite quantizer is a function $Q: A \to B, |\set{Q(x): x \in A}| < \infty$.
\end{definition}
This gives normal definition for discrete random variables and $\infty$ for continuous and mixed random variables

\chapter{Information}

\section{Information}
Not a good question: How much information is there in $X$?
Better questions: The information in $X$ about random variable $Y$?

\begin{definition}
  The (mutual) information given by $X$ about $Y$ is defined as
  \[I(X; Y) \defeq H(Y) - H(Y \mid X)\]
\end{definition}
\begin{definition}
  $Y =$ outcome of a fair $6$-sided die. $X = $ oddity of the outcome. We have \[
  H(Y) = \log_2 6, H(Y \mid X) = \log_2 3 \implies I(X; Y) = \log_2 6 - \log_2 3 = \log_2 2 = 1.  
  \]
\end{definition}

\begin{lemma}
  Suppose $X, Y$ are discrete random variables. Then
  \begin{enumerate}
    \item $I(X; Y) \geq 0$, $= 0$ iff $X, Y$ independent.
    \item We have alternate formulas \begin{align*}
      I(X; Y) & = - \sum_{y} p(y) \log p(y) + \sum_{x, y} p(x, y) \log p(y|x) \\
      & = - \sum_{x, y} p(x, y) \log p(y) + \sum_{x, y} p(x, y) \log p(y|x) \\
      & = \sum_{x, y} p(x, y) \log \frac{p(y | x)}{p(y)} \\
      & = \sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x)p(y)} 
    \end{align*}
    This shows that $I$ is symmetric: $I(X; Y) = I(Y; X)$.
  
    \item From above, \begin{align*}
      I(X; Y) & = -\sum p(x, y)\log p(x) - \sum p(x, y)\log p(y) + \sum p(x, y)\log p(x, y) \\
      & = H(X) + H(Y) - H(X, Y).
    \end{align*}
  
    \item We can view information as an expectation: \[I(X; Y) = \mathbb{E}\left[\log \frac{p(Y \mid X)}{p(Y)}\right] = \mathbb{E}\left[\log \frac{p(X \mid Y)}{p(X)}\right] = \mathbb{E}\left[\log \frac{p(X, Y)}{p(X)p(Y)}\right].\]
    \item We can view information with respect to divergence: \[I(X, Y) = D(p_{XY} \| p_Xp_Y).\]
  \end{enumerate}
\end{lemma}

\begin{remark}
  Alternate notation \(
    I_\phi(X, Y), I_{X; Y}(p), I(p).  
  \)
\end{remark}

What happens if $p(x, y) = 0$, or $p(x) = 0$, or $p(y) = 0$? $p(x) = 0$ or $p(y) = 0 \implies p(x, y) = 0$.

\begin{remark}
  $I(X;Y)$ is possible. Suppose $H(Y) = \infty$ and $Y$ is a function of $X$. Then $I(X; Y) = H(Y) - H(Y \mid X) = \infty - 0 = \infty$.
\end{remark}

Information for more variables \[
  I(X, Y ; V, W, Z) = H(X,Y) - H(X,Y \mid V, W, Z)  
\]

Relations between information entropy
\begin{enumerate}
  \item $I(X; Y) = H(X) - H(X \mid Y) = H(Y) - H(Y \mid X)$.
  \item $I(X; Y) \leq H(X)$, $=$ iff $X$ is a function of $Y$.
  \item $I(X; X) = H(X)$.
  \item $I(X; g(X)) = H(g(X))$.
\end{enumerate}

\section{Conditional Information}
Recall that we have two concepts of conditional entropy.
\[
  H(X \mid Y = y) , H(X \mid Y) = \sum_y p(y) H(X \mid Y = y) 
\]
We are going to use the same approach for conditional information.
\begin{definition}
  Suppose $X, Y, Z$ are two discrete random variables, \[
    I(X; Y | Z = z) = \sum_{x, y} p(x, y) \log \frac{p(xy \mid Z = z)}{p(x| Z = z)p(y| Z = z)}.
  \] \[I(X; Y \mid Z) = \sum_z p(z) I(X; Y \mid Z = z).\]
\end{definition}

\begin{lemma}
  \begin{enumerate}
    \item \(I(X ; Y \mid Z = z) \geq 0\)
    \item \(I(X; Y \mid Z) \geq 0\), \(= 0\) iff $X, Y$ are conditionally independent given $Z$.
    \item \(I(X; Y \mid Z) = H(X \mid Z) - H(X \mid Y,Z).\)
    \item \(I(X; Y, Z) = I(X; Z) + I(X; Y \mid Z)\)
  \end{enumerate}
\end{lemma}

chain rule of conditional information
\[I(X; YZ \mid U) = I(X; Z \mid U) + I(X; Y \mid Z, U).\]

\section{Cryptography From Information Perspective}

$A_X = A_K$ and $|A_X| = |A_K| = 2^N$. $p_K(k) = 2^{-N}, k \in A_K$.

If $|K| < |A_X|$ then the crypto system is not perfect.

Fix $x \in |A_X|$. For each $y$ we have
$P(Y = y|X = x) = P(Y = y) > 0$. Therefore for each $y$, there
must be some key $k \in K$ such that $y = e_K(x)$. It follows that
$|K| \geq |Y|$. The encryption is injective giving $|Y| \geq |A_X|$.

\chapter{Continuous Random Variables}

\section{}

\begin{definition}
  
\end{definition}
We assume alphabet is $\R$, $X$ is absolutely continuous

\begin{example}
  \begin{enumerate}
    \item Uniform \[p(x) = \begin{cases}
      \frac{1}{b-a} & a \leq x \leq b \\
      0 & \text{otherwise}
    \end{cases}\]
    \item Gaussian \[p(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-m)^2}{2\sigma^2}}\]
    \item Laplacian \[p(x) = \frac{1}{\sqrt{2}\sigma}e^{-\frac{\sqrt{2}}{\sigma}|x|}\]
    \item Exponential \[p(x) = \begin{cases}
      \frac{\sqrt{2}}{\sigma}e^{-\frac{\sqrt{2}}{\sigma}|x|} & x \geq 0 \\
      0 & x < 0
    \end{cases}\]
  \end{enumerate}
\end{example}

\begin{definition}
  The \emph{support} of a random variable $X$ or of its probablity distribution is defined by \[S \defeq \set{x: \Pr(X \doteq x \pm \varepsilon) > 0, \forall \varepsilon>0}.\]
\end{definition}

\begin{definition}
  Conditional probability \[\Pr(F \mid X = x) = \frac{\Pr(F, X = x)}{\Pr(X = x)}.\]
  When $X$ is continuous, \[\Pr(F \mid X = x) = \lim_{\delta \to 0} \Pr(F \mid X \doteq x \pm \delta)\]
\end{definition}

Suppose $Y = 3X$ \[
  1 = \Pr(Y = 3 \mid X = 1) = \lim_{\delta = 0} \frac{\Pr(Y = 3 \mid X \doteq 1 \pm \delta)}{\Pr(X \doteq 1 \pm \delta)} = 0, \text{ contradition}
\]
So we use \[
  \Pr(Y \in F \mid X = x) = \lim_{\varepsilon \to 0} \lim_{\delta \to 0} \Pr(Y \in F_\delta \mid X \doteq x \pm \varepsilon) 
\] where \[
  F_\delta = \set{y \mid \norm{y - y'} \text{ for some } y' \in F}.
\]

Generalized sum 






\end{document}
